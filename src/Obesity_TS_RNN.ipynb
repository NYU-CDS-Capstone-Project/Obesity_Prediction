{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import string\n",
    "import random\n",
    "import pdb\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(152)\n",
    "\n",
    "nowPath = os.path.abspath('.')\n",
    "dataPath = nowPath + '//dataSourceCsv'\n",
    "resPath = nowPath + '//resultsRNN'\n",
    "\n",
    "all_train = []\n",
    "all_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pkl_dumper(objct, file_name):\n",
    "    with open(file_name, 'wb+') as f:\n",
    "        pkl.dump(objct, f, protocol=None)\n",
    "\n",
    "def pkl_loader(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        objct = pkl.load(f, encoding = 'bytes')\n",
    "    return(objct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "allColNames = pkl_loader(dataPath + '//allColNames.p')\n",
    "colHeader, colSlot = allColNames['columnHeaders'], allColNames['columnSlots']\n",
    "allTrainTS = pkl_loader(dataPath + '//allTrainTS.p')\n",
    "allIDs = pkl_loader(dataPath + '//allIDs.p')\n",
    "allTgts = pkl_loader(dataPath + '//allTgts.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Target for 5 years old: target_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenTrain:11494, lenTgts:11494\n"
     ]
    }
   ],
   "source": [
    "print('lenTrain:{}, lenTgts:{}'.format(len(allTrainTS), len(allTgts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'avg0to1'), (1, 'avg1to3'), (2, 'avg3to5'), (3, 'avg5to7'), (4, 'avg7to10'), (5, 'avg10to13'), (6, 'avg13to16'), (7, 'avg16to19'), (8, 'avg19to24'), (9, 'avg24to27'), (10, 'avg27to33'), (11, 'avg33to42'), (12, 'avg42to54'), (13, 'avg54to66'), (14, 'avg66to78')]\n"
     ]
    }
   ],
   "source": [
    "print([item for item in enumerate(colSlot)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "newIndex = [i for i, item in enumerate(allTrainTS) if sum(sum(item[:, :9])) != 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dumper(newIndex, dataPath + '//allIndNoZero.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTgts = np.array(allTgts)\n",
    "allTrainTS = np.array(allTrainTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTgts = allTgts[newIndex, 3]\n",
    "allTrainTS = allTrainTS[newIndex]\n",
    "allIDs = np.array(allIDs)\n",
    "allIDs = allIDs[newIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only use 0-24 data, transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTrainTS = allTrainTS[:, :, :9].transpose(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    all_predicted = []\n",
    "    all_label = []\n",
    "    all_lastHid = []\n",
    "    for i, (tsdata, lengths, labels) in enumerate(loader):\n",
    "        tsdata_batch, lengths_batch, label_batch = tsdata.float(), lengths, labels\n",
    "        lastHid, outputs = model(tsdata_batch, lengths_batch)\n",
    "        outputs = F.softmax(outputs, dim=1)\n",
    "        predicted = [x.item() for x in outputs.max(1, keepdim=True)[1]]\n",
    "        all_predicted += predicted\n",
    "        all_label += [x.item() for x in labels]\n",
    "        # print(lastHid.size())\n",
    "        all_lastHid.extend(lastHid.detach().numpy())\n",
    "    return (all_predicted, all_label, all_lastHid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObesityDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tsData, tgt):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.tsData_list, self.target_list = tsData, tgt\n",
    "        assert (len(self.tsData_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tsData_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        Return: a time series data, len(time series data), label\n",
    "        \"\"\"\n",
    "        #tot_cen = self.interact[method](self.cen_1_list[key], self.cen_2_list[key])\n",
    "        tsData = self.tsData_list[key]\n",
    "        label = self.target_list[key]\n",
    "        return [tsData, len(tsData), label]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    \n",
    "    data_list, length_list, label_list = zip(*batch)\n",
    "\n",
    "    # for datum in batch:\n",
    "    #     label_list.append(datum[2])\n",
    "    #     length_list.append(datum[1])\n",
    "    #     data_list.append(datum[0])\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, bidirectional=False):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.rnn = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.linear_1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        if self.bidirectional == True:\n",
    "            hidden = torch.randn(self.num_layers*2, batch_size, self.hidden_size)\n",
    "        else:\n",
    "            hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, tsdata, lengths):\n",
    "        # reset hidden state\n",
    "        #print(c1.size())\n",
    "        batch_size, seq_len, emb_size = tsdata.size()\n",
    "\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        tsdata_rnn_out, hidden = self.rnn(tsdata, self.hidden)\n",
    "        \n",
    "        hidden, _ = torch.max(hidden, dim=0)\n",
    "        \n",
    "        l1_out = self.linear_1(hidden)\n",
    "        l2_out = self.linear_2(l1_out)\n",
    "        \n",
    "        return l1_out, l2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitThreshold = 4000\n",
    "BATCH_SIZE = 10\n",
    "train_dataset = ObesityDataset(allTrainTS[:splitThreshold], allTgts[:splitThreshold])\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = ObesityDataset(allTrainTS[splitThreshold:], allTgts[splitThreshold:])\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset = ObesityDataset(allTrainTS, allTgts)\n",
    "all_loader = torch.utils.data.DataLoader(dataset=all_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for i, (tsdata, lengths, labels) in enumerate(loader):\n",
    "        tsdata_batch, lengths_batch, label_batch = tsdata.float(), lengths, labels\n",
    "        lastHid, outputs = model(tsdata_batch, lengths_batch)\n",
    "        outputs = F.softmax(outputs, dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def model_runner(emb_size=len(colHeader), hidden_size=30, num_layers=1, num_classes=2, bidirectional=True, \\\n",
    "                 learning_rate=2e-4, num_epochs=5, weight_decay=0):\n",
    "    if 'model' in dir():\n",
    "        del(model)\n",
    "        \n",
    "    model = GRU(emb_size=emb_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes, \\\n",
    "                bidirectional=bidirectional)\n",
    "\n",
    "    learning_rate = learning_rate\n",
    "    num_epochs = num_epochs # number epoch to train\n",
    "\n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    # model.cuda()\n",
    "    # criterion.cuda()\n",
    "\n",
    "    print('num_parameters:', sum(p.numel() for p in model.parameters()))\n",
    "    \n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "\n",
    "    train_loss_list = []\n",
    "    val_acc_list = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (tsdata, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            _, outputs = model(tsdata.float(), lengths)\n",
    "            #print(outputs.size())\n",
    "            #print(labels.size())\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                val_acc_list.append(val_acc)\n",
    "                train_loss = loss.item()\n",
    "                train_loss_list.append(train_loss)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Validation Acc: {}'.format(\n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), train_loss, val_acc))\n",
    "    pkl_dumper(train_loss_list, resPath + '/train_loss_list_' + str(hidden_size) + \\\n",
    "               str(learning_rate) + str(weight_decay) + '.p')\n",
    "    pkl_dumper(val_acc_list, resPath + '/val_acc_list_' + str(hidden_size) + \\\n",
    "               str(learning_rate) + str(weight_decay) + '.p')\n",
    "    torch.save(model.state_dict(), resPath + '/model_' + str(hidden_size) + \\\n",
    "               str(learning_rate) + str(weight_decay) + '.model')\n",
    "    return(model, train_loss_list, val_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_parameters: 10892\n",
      "Epoch: [1/30], Step: [101/400], Train Loss: 0.9010709524154663, Validation Acc: 28.449197860962567\n",
      "Epoch: [1/30], Step: [201/400], Train Loss: 0.6787683367729187, Validation Acc: 28.342245989304814\n",
      "Epoch: [1/30], Step: [301/400], Train Loss: 0.7900444269180298, Validation Acc: 30.58823529411765\n",
      "Epoch: [2/30], Step: [101/400], Train Loss: 0.7412386536598206, Validation Acc: 33.2620320855615\n",
      "Epoch: [2/30], Step: [201/400], Train Loss: 0.7214570045471191, Validation Acc: 34.43850267379679\n",
      "Epoch: [2/30], Step: [301/400], Train Loss: 0.7924585938453674, Validation Acc: 35.93582887700535\n",
      "Epoch: [3/30], Step: [101/400], Train Loss: 0.71490079164505, Validation Acc: 37.64705882352941\n",
      "Epoch: [3/30], Step: [201/400], Train Loss: 0.7045150399208069, Validation Acc: 40.42780748663102\n",
      "Epoch: [3/30], Step: [301/400], Train Loss: 0.7342092394828796, Validation Acc: 42.45989304812834\n",
      "Epoch: [4/30], Step: [101/400], Train Loss: 0.7133797407150269, Validation Acc: 44.919786096256686\n",
      "Epoch: [4/30], Step: [201/400], Train Loss: 0.748951256275177, Validation Acc: 42.45989304812834\n",
      "Epoch: [4/30], Step: [301/400], Train Loss: 0.764030396938324, Validation Acc: 49.518716577540104\n",
      "Epoch: [5/30], Step: [101/400], Train Loss: 0.7047091722488403, Validation Acc: 52.72727272727273\n",
      "Epoch: [5/30], Step: [201/400], Train Loss: 0.7109943628311157, Validation Acc: 56.0427807486631\n",
      "Epoch: [5/30], Step: [301/400], Train Loss: 0.7049719095230103, Validation Acc: 57.4331550802139\n",
      "Epoch: [6/30], Step: [101/400], Train Loss: 0.6940864324569702, Validation Acc: 59.786096256684495\n",
      "Epoch: [6/30], Step: [201/400], Train Loss: 0.7455012202262878, Validation Acc: 61.1764705882353\n",
      "Epoch: [6/30], Step: [301/400], Train Loss: 0.7154003977775574, Validation Acc: 65.3475935828877\n",
      "Epoch: [7/30], Step: [101/400], Train Loss: 0.603907585144043, Validation Acc: 66.7379679144385\n",
      "Epoch: [7/30], Step: [201/400], Train Loss: 0.695802628993988, Validation Acc: 70.16042780748663\n",
      "Epoch: [7/30], Step: [301/400], Train Loss: 0.6888526082038879, Validation Acc: 71.55080213903743\n",
      "Epoch: [8/30], Step: [101/400], Train Loss: 0.6810353398323059, Validation Acc: 75.08021390374331\n",
      "Epoch: [8/30], Step: [201/400], Train Loss: 0.6475480794906616, Validation Acc: 74.97326203208556\n",
      "Epoch: [8/30], Step: [301/400], Train Loss: 0.6670523881912231, Validation Acc: 75.40106951871658\n",
      "Epoch: [9/30], Step: [101/400], Train Loss: 0.6460561156272888, Validation Acc: 76.2566844919786\n",
      "Epoch: [9/30], Step: [201/400], Train Loss: 0.6911512017250061, Validation Acc: 78.6096256684492\n",
      "Epoch: [9/30], Step: [301/400], Train Loss: 0.6395422220230103, Validation Acc: 78.71657754010695\n",
      "Epoch: [10/30], Step: [101/400], Train Loss: 0.586276650428772, Validation Acc: 80.64171122994652\n",
      "Epoch: [10/30], Step: [201/400], Train Loss: 0.6705465316772461, Validation Acc: 81.28342245989305\n",
      "Epoch: [10/30], Step: [301/400], Train Loss: 0.5843475461006165, Validation Acc: 80.64171122994652\n",
      "Epoch: [11/30], Step: [101/400], Train Loss: 0.5970224142074585, Validation Acc: 82.13903743315508\n",
      "Epoch: [11/30], Step: [201/400], Train Loss: 0.5444174408912659, Validation Acc: 82.67379679144385\n",
      "Epoch: [11/30], Step: [301/400], Train Loss: 0.6416870355606079, Validation Acc: 82.56684491978609\n",
      "Epoch: [12/30], Step: [101/400], Train Loss: 0.6738582849502563, Validation Acc: 83.42245989304813\n",
      "Epoch: [12/30], Step: [201/400], Train Loss: 0.5874645113945007, Validation Acc: 83.52941176470588\n",
      "Epoch: [12/30], Step: [301/400], Train Loss: 0.5729233026504517, Validation Acc: 83.85026737967914\n",
      "Epoch: [13/30], Step: [101/400], Train Loss: 0.5959054231643677, Validation Acc: 84.06417112299465\n",
      "Epoch: [13/30], Step: [201/400], Train Loss: 0.5762900114059448, Validation Acc: 83.9572192513369\n",
      "Epoch: [13/30], Step: [301/400], Train Loss: 0.5514167547225952, Validation Acc: 83.85026737967914\n",
      "Epoch: [14/30], Step: [101/400], Train Loss: 0.5171666145324707, Validation Acc: 83.31550802139037\n",
      "Epoch: [14/30], Step: [201/400], Train Loss: 0.5373867154121399, Validation Acc: 83.63636363636364\n",
      "Epoch: [14/30], Step: [301/400], Train Loss: 0.5475517511367798, Validation Acc: 83.9572192513369\n",
      "Epoch: [15/30], Step: [101/400], Train Loss: 0.4910343587398529, Validation Acc: 84.06417112299465\n",
      "Epoch: [15/30], Step: [201/400], Train Loss: 0.5067024827003479, Validation Acc: 83.7433155080214\n",
      "Epoch: [15/30], Step: [301/400], Train Loss: 0.5475111603736877, Validation Acc: 83.9572192513369\n",
      "Epoch: [16/30], Step: [101/400], Train Loss: 0.5990661382675171, Validation Acc: 84.38502673796792\n",
      "Epoch: [16/30], Step: [201/400], Train Loss: 0.6408179998397827, Validation Acc: 84.06417112299465\n",
      "Epoch: [16/30], Step: [301/400], Train Loss: 0.504899799823761, Validation Acc: 84.1711229946524\n",
      "Epoch: [17/30], Step: [101/400], Train Loss: 0.5157164335250854, Validation Acc: 84.1711229946524\n",
      "Epoch: [17/30], Step: [201/400], Train Loss: 0.6845638155937195, Validation Acc: 84.1711229946524\n",
      "Epoch: [17/30], Step: [301/400], Train Loss: 0.5294986963272095, Validation Acc: 84.1711229946524\n",
      "Epoch: [18/30], Step: [101/400], Train Loss: 0.5379064679145813, Validation Acc: 84.27807486631016\n",
      "Epoch: [18/30], Step: [201/400], Train Loss: 0.4105023741722107, Validation Acc: 84.38502673796792\n",
      "Epoch: [18/30], Step: [301/400], Train Loss: 0.5341069102287292, Validation Acc: 84.06417112299465\n",
      "Epoch: [19/30], Step: [101/400], Train Loss: 0.5529619455337524, Validation Acc: 84.1711229946524\n",
      "Epoch: [19/30], Step: [201/400], Train Loss: 0.48173221945762634, Validation Acc: 84.1711229946524\n",
      "Epoch: [19/30], Step: [301/400], Train Loss: 0.6250731945037842, Validation Acc: 84.06417112299465\n",
      "Epoch: [20/30], Step: [101/400], Train Loss: 0.5736711621284485, Validation Acc: 84.1711229946524\n",
      "Epoch: [20/30], Step: [201/400], Train Loss: 0.5096232891082764, Validation Acc: 84.1711229946524\n",
      "Epoch: [20/30], Step: [301/400], Train Loss: 0.5359669327735901, Validation Acc: 84.1711229946524\n",
      "Epoch: [21/30], Step: [101/400], Train Loss: 0.4471816420555115, Validation Acc: 84.1711229946524\n",
      "Epoch: [21/30], Step: [201/400], Train Loss: 0.45640143752098083, Validation Acc: 84.1711229946524\n",
      "Epoch: [21/30], Step: [301/400], Train Loss: 0.4497937560081482, Validation Acc: 84.1711229946524\n",
      "Epoch: [22/30], Step: [101/400], Train Loss: 0.5149475932121277, Validation Acc: 84.1711229946524\n",
      "Epoch: [22/30], Step: [201/400], Train Loss: 0.46700024604797363, Validation Acc: 84.1711229946524\n",
      "Epoch: [22/30], Step: [301/400], Train Loss: 0.49451032280921936, Validation Acc: 84.1711229946524\n",
      "Epoch: [23/30], Step: [101/400], Train Loss: 0.6229757070541382, Validation Acc: 84.1711229946524\n",
      "Epoch: [23/30], Step: [201/400], Train Loss: 0.3872872292995453, Validation Acc: 84.1711229946524\n",
      "Epoch: [23/30], Step: [301/400], Train Loss: 0.2994876503944397, Validation Acc: 84.1711229946524\n",
      "Epoch: [24/30], Step: [101/400], Train Loss: 0.539948582649231, Validation Acc: 84.1711229946524\n",
      "Epoch: [24/30], Step: [201/400], Train Loss: 0.6505230665206909, Validation Acc: 84.1711229946524\n",
      "Epoch: [24/30], Step: [301/400], Train Loss: 0.581998884677887, Validation Acc: 84.1711229946524\n",
      "Epoch: [25/30], Step: [101/400], Train Loss: 0.3609652519226074, Validation Acc: 84.1711229946524\n",
      "Epoch: [25/30], Step: [201/400], Train Loss: 0.35566726326942444, Validation Acc: 84.1711229946524\n",
      "Epoch: [25/30], Step: [301/400], Train Loss: 0.534245491027832, Validation Acc: 84.1711229946524\n",
      "Epoch: [26/30], Step: [101/400], Train Loss: 0.630545437335968, Validation Acc: 84.1711229946524\n",
      "Epoch: [26/30], Step: [201/400], Train Loss: 0.5113152265548706, Validation Acc: 84.1711229946524\n",
      "Epoch: [26/30], Step: [301/400], Train Loss: 0.47230973839759827, Validation Acc: 84.1711229946524\n",
      "Epoch: [27/30], Step: [101/400], Train Loss: 0.6492185592651367, Validation Acc: 84.1711229946524\n",
      "Epoch: [27/30], Step: [201/400], Train Loss: 0.6056214570999146, Validation Acc: 84.1711229946524\n",
      "Epoch: [27/30], Step: [301/400], Train Loss: 0.5654165148735046, Validation Acc: 84.1711229946524\n",
      "Epoch: [28/30], Step: [101/400], Train Loss: 0.6680651903152466, Validation Acc: 84.1711229946524\n",
      "Epoch: [28/30], Step: [201/400], Train Loss: 0.5583590269088745, Validation Acc: 84.1711229946524\n",
      "Epoch: [28/30], Step: [301/400], Train Loss: 0.7879073619842529, Validation Acc: 84.1711229946524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29/30], Step: [101/400], Train Loss: 0.36511683464050293, Validation Acc: 84.1711229946524\n",
      "Epoch: [29/30], Step: [201/400], Train Loss: 0.48883137106895447, Validation Acc: 84.1711229946524\n",
      "Epoch: [29/30], Step: [301/400], Train Loss: 0.45888596773147583, Validation Acc: 84.1711229946524\n",
      "Epoch: [30/30], Step: [101/400], Train Loss: 0.388408362865448, Validation Acc: 84.1711229946524\n",
      "Epoch: [30/30], Step: [201/400], Train Loss: 0.39059823751449585, Validation Acc: 84.1711229946524\n",
      "Epoch: [30/30], Step: [301/400], Train Loss: 0.43091583251953125, Validation Acc: 84.1711229946524\n"
     ]
    }
   ],
   "source": [
    "model1, train_loss_l1, val_acc_l1 = model_runner(learning_rate = 2e-6, num_epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, label, hid = predictor(all_loader, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'pred':pred, 'label':label, 'hid':hid}\n",
    "pkl_dumper(results, resPath + '//results1118.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
